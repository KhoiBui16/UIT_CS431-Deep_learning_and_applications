import json
import gradio as gr
import torch
import os
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
from vision import VisionModule  # <--- Import má»›i

from agent import ToolUseAgent
from tools import TOOLS_SCHEMA

MODEL_OPTIONS = {
    "Qwen Agent": "/home/manh/Projects/temp/CS431/src_agent/agent_model_weights/checkpoint-318",
    "Vietnamse Qwen 2.5 Math (1.5B)": "piikerpham/Vietnamese-Qwen2.5-math-1.5B", 
    "Qwen 2.5 Math (1.5B)": "Qwen/Qwen2.5-Math-1.5B-Instruct",
    "Qwen 2.5 Math (7B)": "Qwen/Qwen2.5-Math-7B-Instruct",
}

current_model = None
current_tokenizer = None
current_agent = None
loaded_model_name = ""
vision_module = VisionModule()

def clean_memory():
    """HÃ m dá»n dáº¹p VRAM triá»‡t Ä‘á»ƒ"""
    global current_model, current_tokenizer, current_agent
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()

def load_model_pipeline(model_key):
    global current_model, current_tokenizer, current_agent, loaded_model_name
    
    if loaded_model_name == model_key and current_agent is not None:
        return f"âœ… Model '{model_key}' Ä‘Ã£ sáºµn sÃ ng!"

    print(f"ðŸ”„ Äang chuyá»ƒn Ä‘á»•i sang model: {model_key}...")
    
    if current_model is not None:
        del current_model
        del current_tokenizer
        del current_agent
        clean_memory()

    model_path = MODEL_OPTIONS[model_key]
    try:
        print(f"â³ Äang load tá»«: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        if "7B" in model_key or "7b" in model_path.lower():
            print("âš ï¸ PhÃ¡t hiá»‡n Model lá»›n (7B). Äang báº­t cháº¿ Ä‘á»™ 4-bit Quantization Ä‘á»ƒ tiáº¿t kiá»‡m VRAM...")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",
                quantization_config=quantization_config, 
                trust_remote_code=True
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
        
        current_model = model
        current_tokenizer = tokenizer
        current_agent = ToolUseAgent(model, tokenizer, tools_metadata=TOOLS_SCHEMA)
        loaded_model_name = model_key
        
        print(f"âœ… Load thÃ nh cÃ´ng: {model_key}")
        return f"âœ… ÄÃ£ chuyá»ƒn sang: {model_key}"
        
    except Exception as e:
        print(f"âŒ Lá»—i load model: {e}")
        return f"âŒ Lá»—i: {str(e)}"

def solve_math_problem(model_select, question, image_path, show_reasoning, temperature, max_tokens):
    global current_agent, loaded_model_name, vision_module
    
    reasoning_display = ""
    full_question = question
    
    if image_path is not None:
        if current_model is not None:
            print("âš ï¸ Táº¡m thá»i unload Math Model Ä‘á»ƒ cháº¡y Vision...")
            clean_memory()
            
        status_msg = "ðŸ‘ï¸ Äang Ä‘á»c áº£nh vá»›i Vintern-1B..."
        print(status_msg)
        reasoning_display += f"### ðŸ‘ï¸ Xá»­ lÃ½ HÃ¬nh áº£nh (Vintern-1B)\n"
        
        try:
            extracted_text = vision_module.extract_text_from_image(image_path)
            reasoning_display += f"> **Ná»™i dung trÃ­ch xuáº¥t:**\n{extracted_text}\n\n---\n"
            
            full_question = f"{extracted_text}\n\n{question}"
        except Exception as e:
            reasoning_display += f"> âŒ Lá»—i Ä‘á»c áº£nh: {str(e)}\n\n---\n"
    
    needs_reload = False
    if loaded_model_name != model_select: needs_reload = True
    try:
        current_model.device 
    except:
        needs_reload = True
        
    if needs_reload:
        status = load_model_pipeline(model_select)
        if "Lá»—i" in status: return status, reasoning_display

    if not current_agent: return "Lá»—i: KhÃ´ng thá»ƒ khá»Ÿi táº¡o Agent.", reasoning_display
    if not full_question.strip(): return "Vui lÃ²ng nháº­p cÃ¢u há»i hoáº·c upload áº£nh.", reasoning_display

    current_agent.generation_cfg = {
        "max_new_tokens": max_tokens,
        "temperature": temperature,
        "do_sample": True if temperature > 0 else False,
    }

    try:
        print(f"ðŸ¤– Agent Ä‘ang suy luáº­n vá»›i model: {loaded_model_name}")
        conversations, final_answer = current_agent.inference(full_question)
        
        if show_reasoning:
            step_count = 1
            for msg in conversations:
                role = msg['role']
                content = str(msg['content'])
                
                if role == 'assistant':
                    if "<tool_call>" in content:
                        parts = content.split("<tool_call>")
                        thought = parts[0].strip()
                        tool_code = parts[1].replace("</tool_call>", "").strip()
                        
                        reasoning_display += f"### ðŸ§  BÆ°á»›c {step_count}: Suy luáº­n\n"
                        if thought: reasoning_display += f"{thought}\n\n"
                        reasoning_display += f"**âš¡ HÃ nh Ä‘á»™ng:**\n```json\n{tool_code}\n```\n\n"
                        step_count += 1
                    else:
                        if content.strip() != final_answer.strip():
                            reasoning_display += f"### ðŸ§  BÆ°á»›c {step_count}: Suy luáº­n\n{content}\n\n"
                            step_count += 1

                elif role == 'tool':
                    clean_res = content.replace("<tool_response>", "").replace("</tool_response>", "").strip()
                    reasoning_display += f"### ðŸ”§ Káº¿t quáº£ CÃ´ng cá»¥\n> {clean_res}\n\n---\n"

        if not final_answer:
            final_answer = conversations[-1]['content']

        return final_answer, reasoning_display

    except Exception as e:
        return f"Lá»—i há»‡ thá»‘ng: {str(e)}", reasoning_display

# --- GRADIO UI ---
css = """
#reasoning_box { background-color: #f9f9f9; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; max-height: 500px; overflow-y: auto; }
#status_box { font-weight: bold; color: #2e7d32; }
"""

with gr.Blocks(title="Math Agent + Vintern Vision", theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown("# ðŸ§® Há»‡ thá»‘ng Giáº£i ToÃ¡n Äa PhÆ°Æ¡ng Thá»©c (Vintern + Qwen)")
    
    with gr.Row():
        with gr.Column(scale=4):
            with gr.Group():
                gr.Markdown("### 1. Cáº¥u hÃ¬nh Model")
                model_selector = gr.Dropdown(
                    choices=list(MODEL_OPTIONS.keys()),
                    value="My Finetune (Checkpoint 318)", 
                    label="Math Agent Model",
                    interactive=True
                )
                load_status = gr.Textbox(label="Tráº¡ng thÃ¡i", value="Sáºµn sÃ ng...", elem_id="status_box", interactive=False)
            
            with gr.Group():
                gr.Markdown("### 2. Nháº­p Äá» BÃ i")
                # DÃ¹ng type="filepath" Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i hÃ m load_image cá»§a Vintern
                image_input = gr.Image(type="filepath", label="ðŸ“¸ Upload áº£nh bÃ i toÃ¡n")
                question_input = gr.Textbox(lines=3, placeholder="Nháº­p thÃªm yÃªu cáº§u (VD: Giáº£i chi tiáº¿t bÃ i toÃ¡n trÃªn)...", label="CÃ¢u há»i bá»• sung")
            
            with gr.Accordion("âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao", open=False):
                temperature = gr.Slider(0.0, 1.0, 0.5, label="Temperature")
                max_tokens = gr.Slider(128, 2048, 1024, label="Max Tokens")
                show_reasoning = gr.Checkbox(True, label="Hiá»‡n suy luáº­n")
            
            solve_btn = gr.Button("ðŸš€ GIáº¢I BÃ€I NGAY", variant="primary", size="lg")

        with gr.Column(scale=5):
            gr.Markdown("### ðŸ Káº¿t quáº£ cuá»‘i cÃ¹ng")
            answer_output = gr.Textbox(label="", interactive=False, lines=3)
            gr.Markdown("### ðŸ§  QuÃ¡ trÃ¬nh suy luáº­n (Vision -> Thought -> Tools)")
            reasoning_output = gr.Markdown(elem_id="reasoning_box")

    model_selector.change(
        fn=load_model_pipeline,
        inputs=[model_selector],
        outputs=[load_status]
    )

    solve_btn.click(
        fn=solve_math_problem,
        inputs=[model_selector, question_input, image_input, show_reasoning, temperature, max_tokens],
        outputs=[answer_output, reasoning_output]
    )
    
    demo.load(fn=load_model_pipeline, inputs=[model_selector], outputs=[load_status])

if __name__ == "__main__":
    demo.launch(share=True)